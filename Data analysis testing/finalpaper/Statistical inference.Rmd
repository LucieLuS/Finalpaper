---
title: "Statistical inferences"
author: "Lucie Lu"
date: "May 13, 2018"
output: pdf_document
---

```{r global_options, include=FALSE, cache=FALSE}
## To make the pdf file do
## render("exploration4.Rmd",output_format=pdf_document())

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  message=FALSE,
  comment=NA)

```


```{r initialize,echo=FALSE, message=FALSE, warning=FALSE}
##First, just setup the R environment for today:
if(!file.exists('figs')) dir.create('figs')

options(SweaveHooks=list(fig=function(){
			   par(mar=c(3.5, 3, 1.1, 0),
			       pty="s",
			       mgp=c(1.5,0.5,0),
			       oma=c(0,0,0,0))},
			 echo=function(){options(continue=" ") ##Don't show "+" prompts,
			 options(prompt=" ")
			 }),
	digits=4,
	scipen=8,
	width=132
	)
options(error=function(){options(prompt="> ",continue="+ ");NULL})


#To prepare the environment
library(readstata13)
library(dplyr)
library(ggplot2)
#install.packages("ggrepel")
library(ggrepel)
library(stargazer)

#install.packages("devtools")
library(devtools)

library(foreign)
#install.packages("gplots")
library(gplots)

library(lmtest)
library(sandwich)

library(tidyverse)

library(MASS)
library(robustbase)

#install.packages("rmngb")
library(rmngb)
library(here)
```

```{r set up the data, echo=FALSE, message=FALSE, warning=FALSE, include=F}
getwd()
set_here(path=".", verbose=T)
here()

list.files(path=".")
load("mydata_5collapseyear.rda")
```

##Checking unbiasedness and consistency of the estimates in the OLS model
```{r check OLS estimates unbiasedness, results='hide', eval = TRUE, echo = FALSE, message = FALSE, warning=FALSE, include=F}

OLS_5_0Bs <- lm(Tariff_c ~ Regime_Bi_c, data=mydata_5)
summary(OLS_5_0Bs)$coefficient[2,1] #-4.29

OLS_5_5Bs <- lm(Tariff_c ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5)
summary(OLS_5_5Bs)$coefficient[2,1] #-2.84

############


#z is the experiment, shuffle the mydata4$Regime_l1_Bi variable to create a pretend experiment

set.seed(2018)

mydata_5test <- mydata_5

#rm(mydata_5test$a)
#mydata_5test %>% select(-a)
mydata_5test$Z <- sample(rep(c(0,1), each = 1, len = 81))


```

```{r set up potential outcomes, eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}
#Define a potential outcome?

#Invent TrueATE

#First come up with a way for covariates to be related to the potential outcome under control (y0). This produces background noise in the outcome (i.e. variation that has nothing to do with the treatment assignment or treatment effect.)
mydata_5test$y0 <- with(mydata_5test, 3 + .05*GDP_c + .4*EC_c - .8*WTO_c) + 
runif(n = 81, min=min(mydata_5test$Tariff_c),max=max(mydata_5test$Tariff_c)) 
#To create background noise...no effect at all.


lm_test2 <- lm(y0 ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5test)
summary(lm_test2)$r.squared #0.98

mydata_5test$y1 <- with(mydata_5test, y0 + .08*sd(y0))

trueATE <- mean(mydata_5test$y1) - mean(mydata_5test$y0)
trueATE #11.43
.08*sd(mydata_5test$y0) #11.43

#Now, I know the trueATE: 11.43

## Observed outcome: y1 among the treated, y0 among the controls
mydata_5test$Y <- with(mydata_5test, Z*y1 + (1-Z)*y0)

lm_test3 <- lm(Y ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5test) #also use the covariates
summary(lm_test3)$r.squared #0.98


## Notice that this includes all of the true covariates, just not in the correct function, plus it includes some irrelevant covariates
estATEbest_5_5Bs <- coef(OLS_5_5Bs)[["Regime_Bi_c"]] #-2.84
estATEbest_5_5Bs ##-2.84

##Unbiased estimates: includes nothing (no covariates)
estATEunbiased_5_5Bs <- coef(OLS_5_0Bs)[["Regime_Bi_c"]]

estATEunbiased_5_5Bs ##-4.29
```

```{r biassketch, results='hide', eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}
set.seed(2018)

## Bias refers to a relationship between the repeated operation of a procedure and a truth.
## So we have to invent a truth (that is why we **created** potential outcomes for all units ---
## ordinarily we would not observe these quantities).

#Repeat the experiment. Each repetition reveals a different potential outcome for each person. Calculate the estimates from the two different estimators.
#This should show (1) bias and (2) whether one or the other is better in mean squared error terms. What would I do to show that one or another is consistent
#--- especially that one or the other converges to the truth more or less quickly as sample sizes increase?


newEstimate<-function(obsz,dat){
  newExperiment<-function(z){
    #sample(z) is permutating z
    sample(z)
  }
  #create newz
  dat$newz <- newExperiment(obsz)
  ##names(newz) <- row.names(dat)
  #create newY
  dat$newY <- with(dat,newz*y1 + (1-newz)*y0)
  #use three methods to create ATE
  theestATEbest <- coef(lm(newY~newz + Regime_Bi_c + GDP_c + EC_c + WTO_c, data=dat))[["newz"]]
  theestATEunbiased <- coef(lm(newY~newz,data=dat))[["newz"]]
  ## Another method of using covariates
  dat$e1 <- residuals(lm(newY~Regime_Bi_c + GDP_c + EC_c + WTO_c, data=dat))
  theestATEbest2 <- coef(lm(e1~newz,data=dat))[[2]]
  return(c(bestATE=theestATEbest,
           unbiasedATE=theestATEunbiased,
           bestATE2=theestATEbest2))
}

## test, maybe a good idea to test it after we create a function
## newEstimate(obsz=wrkdat$Z,dat=wrkdat)

## An unbiased estimator is one where E[estimator]=Truth. Hmmm.. Is this the right test?
set.seed(1234568)
estdists <- replicate(10000,newEstimate(obsz=mydata_5test$Z,dat=mydata_5test))

```

```{r compare trueATE and the other types of ATE,  eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}

trueATE #11.36
##sampdistmeans<-apply(estdists,1,mean)
#meanATE
sampmeans <- apply(estdists[1:3,],1,mean)
#sampledistmeans: very close to the trueATE
sampmeans

#absolute difference between the estimated mean and the true ATE

bias <- abs(trueATE-apply(estdists[1:3,],1,mean)) # abs(): absoluate positive values: the differences between trueATE and the estimates
apply(estdists[1:3,],1,function(x){ mean( abs(x - trueATE) ) } )
bias
#the standard deviation of the ATE means

##Finding the standard deviation of the estimated ATEs allows us to see if our estimators are efficient.
sd <- apply(estdists[1:3,],1,sd)
sd

#the RMSE of the difference in the true ATE and the estimated ATE

##The RMSE allows us to further assess if the estimators are biased becuase it overesstimates the presence of bias. (to verify)

MSE <- apply(estdists[1:3,],1,function(x){ mean( ( x - trueATE)^2 ) })
MSE
```

```{r trueATE, sampmeans etc, eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}

#install.packages("xtable",repos = "http://cran.us.r-project.org")
library(xtable)

sim <- as.matrix(rbind(trueATE, sampmeans, bias, sd, MSE))
#rownames(rcor) <- names
simtab<-round(sim,2)
simtab
xtable(simtab)
```

```{r xtable print trueATE, sampmeans etc, results = "asis", message = FALSE, eval=T, echo=F, warning = FALSE}
print(xtable::xtable(sim, caption = "Simulation results from different estimates for OLS model"),type = "latex",
      html.table.attributes="border=1")
```


```{r plot the bias, RMSE and consistency, results='asis', eval=T, echo=F, message = FALSE, cache=TRUE}

plot(density(estdists["bestATE",]),ylim=c(0,.25), main = "Simulation results from estimates for different OLS models")
rug(estdists["bestATE",],col="black",line=0)
lines(density(estdists["unbiasedATE",]),col="blue")
rug(estdists["unbiasedATE",],col="blue",line=.5)
lines(density(estdists[3,]),col="orange")
abline(v=trueATE)

sampdistmeans<-apply(estdists,1,mean)
points(c(sampdistmeans[1:3],estATEunbiased_5_5Bs,estATEbest_5_5Bs), rep(0,5),
       pch=c(17,17,17,2,2),
       cex=1,col=c("black","blue","orange","green","red"))

##bestATE and bestATE2 have a little bias, but it is consistent; 
##unbiasedATE is unbiased, but it is very inefficient and inconsistent.

##We can reduce bias only at a potential increase in variance.

```

The *bestATE* is the estimaor in a *lm* function with all the relevant covariates in the model. The *unbiasedATE* is the estimaor in a *lm* function with no covariates in the model at all. The *bestATE2* is the estimaor in a residual-based function.  

To assess biasedness, I compare sample means of the estimators with the true means I created in the simulation test. In a simple way, I can check the thrid row bias (denoted as the absoluate positive values: the differences between trueATE and the means of the estimates). All of the three estimators are close to the true mean (`r trueATE`) I created in the simulation test. This suggest all three of them are (pretty much) unbiased. In fact, the *bestATE* has the lowest bias out of the three. Its absolute distance to the true mean is the smallest one, with bias equals to `r bias[1]`. The *bestATE2* is slightly biased (with biase `r bias[3]`).  

To assess consistency or efficiency, the MSEs for bestATE and bestATE2 are relatively the same, but the bestATE2 behave slightly better (`r MSE[3]` < `r MSE[1]`). The estimators *bestATE* and *bestATE2* are efficient, compared to the unbiasedATE. The standard error for unbiasedATE is very high (`r sd[2]`), suggesting that this estimate is very inefficient and inconsistent. It does not converge to the true mean at all. From this simulation test, because the bestATE has the lowest bias and is the most efficient one, this estimate is preferred. 


##P-values from the permutation test
```{r randomization and p-value for lm, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}


OLS_5_5Bs <- lm(Tariff_c ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5)
simlmtest_lm5Bs <- summary(OLS_5_5Bs)$coef[2,1]
simlmtest_lm5Bs ##-2.84

#Imagine we re-run the "experiment"
newExp<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lm(y ~ shuffledz+ GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

#set.seed(19900814)
expBeta_lm5Bs <- replicate(10000,newExp(y=mydata_5test$Tariff_c))
mean(expBeta_lm5Bs)  #0.053 treatment effect under null

plot(density(expBeta_lm5Bs)) #plot the sampling distribution under null hypothesis
rug(mean(expBeta_lm5Bs), 1, col="blue")
abline(v=simlmtest_lm5Bs,lty=2, col="red")

#One-sided p-value
pvalue_lm_os <- as.numeric(as.matrix(summary(expBeta_lm5Bs > simlmtest_lm5Bs)))/length(expBeta_lm5Bs)
pvalue_lm_os #0.1134

#Two-sided p-value
pvalue_lm_ts <- 2*min(mean(expBeta_lm5Bs>=simlmtest_lm5Bs),mean(expBeta_lm5Bs<=simlmtest_lm5Bs)) #0.2268
#pretty close to the two-sided p-value obtained in the lm (0.28). 
#p_lm <- summary(OLS_5_5Bs)$coef[2,4] 


#In this case the standard assumption that the statistic follows a t distribution gives a p-value of 0.23 which is in quite good agreement with the permutation test (0.28). But we wouldn't necessarily know beforehand that they would agree.

```
```{r P-value distribution with Permutation in 50-times simulation, results='asis', echo = FALSE, message = FALSE, warning=FALSE}

newExp<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lm(y ~ shuffledz+ GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

set.seed(19900814)
#expBeta_lm5Bs <- replicate(10000,newExp(y=mydata_5test$Tariff_c))

#mean(expBeta_lm5Bs)#0.053

pvalue_lm <- function(siml,obsTestStat){
  ## return a p-value
  refDistNull<-replicate(siml,newExp(y=mydata_5test$Tariff_c))
  pTwoSided <- 2*min(mean(refDistNull>=obsTestStat),mean(refDistNull<=obsTestStat))
  return(pTwoSided)
}

pvalue_lm (siml=10000, obsTestStat=simlmtest_lm5Bs)

store_pvalue_lm <- replicate(50, 
pvalue_lm (siml=10000, obsTestStat=simlmtest_lm5Bs))

hist(store_pvalue_lm, xlim=c(0.05, 0.3), main=paste("P-value distribution with Permutation in 50-times simulation"))

```

```{r error rate for lm function, results='hide', echo = FALSE, message = FALSE, warning=FALSE}

#Codes to assess the false positive rate of the canned lm function

err_rate<- function(y){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    sim_ps <- summary(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_ps)
}

set.seed(19900814)
results_lm5Bs <- replicate(1000, 
err_rate(y=mydata_5test$Tariff_c))

mean(results_lm5Bs <= .05, na.rm=T) #0.0674

falsepos_lm5Bs <- as.numeric(as.matrix(summary(results_lm5Bs < .05))[3,1])/length(results_lm5Bs) #0.0674

falsepos_lm5Bs

#The false positive rate is 0.067. If we run it a couple of times, the false positive rates are slightly different, but they are around 0.06. The built-in p-value does not have a valid false positive rate, which does not come close to the nominal false positive rate.

#How to interpret this: you create a null effect knowing that your null hypothesis is true. Then the false positive rate 0.05, which means 5 out of 100 times, the test faslely reject the nulls (knowing the null is true but you still reject it). If the false positive rate is close to 0.05, it means the test fulfills its promises.

```
```{r plotting valid test for lm, results='asis', echo = FALSE, message = FALSE, warning=FALSE}


set.seed(800814)
nSims <- 1000

results_lm5Bs <- replicate(1000, 
err_rate(y=mydata_5test$Tariff_c))#get the p-value and store it
#Check power by summing significant p-values and dividing by number of simulations

#The frequency of p-values, or the type 1 error is 0.043. When there is no effect (in the simulation), we can see the p-value is uniformally distributed under the null. This is a valid test because we have set the type 1 error as 0.05, meaning that we can use this test to conclude we have 0.05 chance to make an error saying that there is a true effect where there is actually no true effect.


bars<-20

op <- par(mar = c(5,7,4,4)) #change white-space around graph
hist(results_lm5Bs, breaks=20, xlab="P-values", ylab="number of p-values\n", axes=T,
     main=paste("P-value Distribution under Null Effect in the Simulation in lm Canned Function"),
     col="grey", xlim=c(0,1),  ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1))
abline(h=nSims/bars, col = "red", lty=3)


#When there is no true effect, p-values are what is called 'uniformly distributed under the null'. The p-value distribution is basically flat. Every p-value is equally likely when the null hypothesis is true, and every bar in the graph will contain 5% of all the p-values (as indicated by the dotted red line). The first bar is the false positive rate, which is slightly higher than 0.05.
```


```{r randomization and p-value for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

set.seed(19900814)
lmrob_5_5Bs <- lmrob(Tariff_c ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5, control = lmrob.control(max.it = 100))
summary(lmrob_5_5Bs)$coefficient #-3.31

simlmrobtest_5Bs <- summary(lmrob_5_5Bs)$coefficient[2,1]
simlmrobtest_5Bs #-3.31

#Imagine we re-run the "experiment" by shuffling the country/treatment
newExp_lmrob<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

set.seed(19900814)
expBeta_lmrob_5Bs <- replicate(10000, newExp_lmrob(y=mydata_5test$Tariff_c))
mean(expBeta_lmrob_5Bs)

plot(density(expBeta_lmrob_5Bs)) #plot the null distribution
rug(mean(expBeta_lmrob_5Bs), 1, col="blue")
abline(v=simlmrobtest_5Bs,lty=2, col="red")

#One-sided p-value
pvalue_lm <- as.numeric(as.matrix(summary(expBeta_lmrob_5Bs < simlmrobtest_5Bs)))/length(expBeta_lmrob_5Bs)
pvalue_lm #0.0325

#Two-sided p-value
2*min(mean(expBeta_lmrob_5Bs>=simlmrobtest_5Bs),mean(expBeta_lmrob_5Bs<=simlmrobtest_5Bs)) #0.065
#different from the two-sided p-value obtained in the lmrob canned function (0.04).

```

```{r error rate for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

#Codes to assess the false positive rate of the canned lmrob function

err_ratetlmrob<- function(y){
    require(robustbase)
    shuffledz <- sample(mydata_5test$Z) 
  sim_psrob <- summary(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_psrob)
}

set.seed(23358243)


nSims <- 1000

results_lmrob5Bs <- replicate(nSims, err_ratetlmrob(y=mydata_5test$Tariff_c))#get the p-value and store it

falsepos_lmrob5Bs <- mean(results_lmrob5Bs <= .05, na.rm = T) #0.055

falsepos_lmrob5Bs
#The Type 1 error, the false positive is 0.055.


#Check power by summing significant p-values and dividing by number of simulations

#The frequency of p-values, or the type 1 error is 0.055. When there is no effect (in the simulation), we can see the p-value is uniformally distributed under the null. This is a valid test because we have set the type 1 error as 0.05, meaning that we can use this test to conclude we have 0.05 chance to make an error saying that there is a true effect where there is actually no true effect.

```

```{r plot error rate for lmrob, results='asis', echo = FALSE, message = FALSE, warning=FALSE}

bars<-20

op <- par(mar = c(5,7,4,4)) #change white-space around graph
hist(results_lmrob5Bs, breaks=20, xlab="P-values", ylab="number of p-values\n", axes=T,
     main=paste("P-value distribution under null effect in the simulation in lmrob"),
     col="grey", xlim=c(0,1),  ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1))
abline(h=nSims/bars, col = "red", lty=3)

#When there is no true effect, p-values are what is called 'uniformly distributed under the null'. The p-value distribution is basically flat. Every p-value is equally likely when the null hypothesis is true, and every bar in the graph will contain 5% of all the p-values (as indicated by the dotted red line).
```


##Power and effect size
```{r effectsize and power for lm, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

## doParallel will not work on all machines. 
library(foreach)
library(doParallel)
cl <- makeCluster(2) ## 2 cores on my machine
registerDoParallel(cl)

set.seed(800814)

power_fn<- function(outcome,effect){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    newoutcome <- outcome - shuffledz*effect
    sim_p<- summary(lm(newoutcome ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_p)
}


#effectsize <- seq(0.1,5, by=0.3)
#results_sim_p <- replicate(1000, power_fn(outcome=mydata4_small$Tariff, effect=0.5))

#somepower <- mean(results_sim_p <= .05) #power
#somepower

######

effectsize <- seq(0.1,5, by=0.3)
somepower <- sapply(seq(0.1,5, by=0.3), function(effectSize){
                     results_sim_p <- replicate (1000, power_fn(outcome=mydata_5test$Tariff_c, effect=effectSize))
                    
                     power <- (mean(results_sim_p <= .05))
                      return(power)
                  })

rbind(effectsize, somepower)

```


```{r effectsize and power for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

## doParallel will not work on all machines. 
library(foreach)
library(doParallel)
cl <- makeCluster(2) ## 2 cores on my machine
registerDoParallel(cl)

set.seed(800814)

power_fn_lmrob<- function(outcome,effect){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    newoutcome <- outcome - shuffledz*effect
    sim_p<- summary(lmrob(newoutcome ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_p)
}

#effectsize <- seq(0.1,5, by=0.3)
#results_sim_p <- replicate(1000, power_fn(outcome=mydata4_small$Tariff, effect=0.5))

#somepower <- mean(results_sim_p <= .05) #power
#somepower

######

effectsize <- seq(0.1,5, by=0.3)

somepower_lmrob <- sapply(seq(0.1,5, by=0.3), function(effectSize){
                     results_sim_p <- replicate (1000, power_fn_lmrob(outcome=mydata_5test$Tariff_c, effect=effectSize))
                    
                     power <- (mean(results_sim_p <= .05, na.rm = T))
                      return(power)
                  })

somepower_lmrob

```

```{r, prepare table power and effectsize, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

#tablepowers <- rbind(effectsize, somepower, somepower_lmrob)

#install.packages("xtable",repos = "http://cran.us.r-project.org")
library(xtable)

tablepowers <- as.matrix(rbind(effectsize, somepower, somepower_lmrob))
xtable(tablepowers)

```

```{r, print table power and effectsize, results='asis', echo = FALSE, message = FALSE, warning=FALSE}
print(xtable::xtable(tablepowers, caption = "Simulation results of Power and Effect Size for lm and lmrob model"),type = "latex",
      html.table.attributes="border=1")
```

```{r, plot power and effectsize, results='asis', echo = FALSE, message = FALSE, warning=FALSE}

powersdata <- data.frame(effectsize, somepower, somepower_lmrob)

stacked <- with(powersdata,
                data.frame(
                  power = c(somepower, somepower_lmrob),
                  variable = factor(rep(c("powerlm", "powerlmrob"),
                    each = NROW(powersdata))),
                      effectsize = rep(effectsize, 2)))

ggplot(data=stacked, aes(x=effectsize, y=power, colour=variable))+
geom_line()+
geom_hline(yintercept=0.8, linetype="dashed", color="black")+
  labs(title="Power and Effect Size in Simulations", x="Effect Size", y = "Power") +
  theme_classic()

```
*Power analysis of two models*:

We can use simulations to estimate the statistical power of a model. The statistical power is the probability of observing a statistically significant result, if there is a true effect. When there is an effect, I hope that my statistical test is able to detect it. This denotes to high power in my study.   

Cohen describes effect size as "the degree to which the null hypothesis is false." In this simulation test, I generate different hypothetical effect sizes (from 0.1 to 5), and I calculate the number of p-values that are are lower than 0.05 ("reject the null") when I know there is a true effect (the null is false). When the effect size increases, the powers in both functions also increase. 


For a given sample size, the *lmrob* model has larger statistical power given an effect size. As effect size increases, the power of the *lmrob* model is also increasing faster than that of the *lm* model. This is probably due to a relatively small sample size in this study (81 countries). To achieve an ideal 80% statistical power, the *lmrob* model requires an effect size larger than 5. 80% statistical power essentially means when there is a true effect, there is 80 percent that I will observe a signifiant effect. For this *lm* model, I need a bigger effect size to achieve the same level of power as *lmrob* model requires. 