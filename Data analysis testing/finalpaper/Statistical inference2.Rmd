---
title: "Statistical inferences"
author: "Lucie Lu"
date: "May 13, 2018"
output: pdf_document
---

```{r global_options, include=FALSE, cache=FALSE}
## To make the pdf file do
## render("exploration4.Rmd",output_format=pdf_document())

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  message=FALSE,
  comment=NA)

```


```{r initialize,echo=FALSE, message=FALSE, warning=FALSE}
##First, just setup the R environment for today:
if(!file.exists('figs')) dir.create('figs')

options(SweaveHooks=list(fig=function(){
			   par(mar=c(3.5, 3, 1.1, 0),
			       pty="s",
			       mgp=c(1.5,0.5,0),
			       oma=c(0,0,0,0))},
			 echo=function(){options(continue=" ") ##Don't show "+" prompts,
			 options(prompt=" ")
			 }),
	digits=4,
	scipen=8,
	width=132
	)
options(error=function(){options(prompt="> ",continue="+ ");NULL})


#To prepare the environment
library(readstata13)
library(dplyr)
library(ggplot2)
#install.packages("ggrepel")
library(ggrepel)
library(stargazer)

#install.packages("devtools")
library(devtools)

library(foreign)
#install.packages("gplots")
library(gplots)

library(lmtest)
library(sandwich)

library(tidyverse)

library(MASS)
library(robustbase)

#install.packages("rmngb")
library(rmngb)
library(here)

devtools::install_github("ropenscilabs/gramr")
library("gramr")
#write_good_ip()
```


```{r set up the data, echo=FALSE, message=FALSE, warning=FALSE, include=F}
getwd()
set_here(path=".", verbose=T)
here()

list.files(path=".")
load("mydata_5collapseyear.rda")
```

##Checking unbiasedness and consistency of the estimates in the OLS model
```{r check OLS estimates unbiasedness, results='hide', eval = TRUE, echo = FALSE, message = FALSE, warning=FALSE, include=F}

OLS_5_0Bs <- lm(Tariff_c ~ Regime_Bi_c, data=mydata_5)
summary(OLS_5_0Bs)$coefficient[2,1] #-4.29

OLS_5_5Bs <- lm(Tariff_c ~ Regime_Bi_c + log(GDP_c) + EC_c + WTO_c, data=mydata_5)

summary(OLS_5_5Bs)$coefficient[2,1] #-2.84

############


#z is the experiment, shuffle the mydata4$Regime_l1_Bi variable to create a pretend experiment

set.seed(2018)

mydata_5test <- mydata_5

#rm(mydata_5test$a)
#mydata_5test %>% select(-a)
mydata_5test$Z <- sample(rep(c(0,1), each = 1, len = 81))


```

```{r set up potential outcomes, eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}
#Define a potential outcome?

#Invent TrueATE

#First come up with a way for covariates to be related to the potential outcome under control (y0). This produces background noise in the outcome (i.e. variation that has nothing to do with the treatment assignment or treatment effect.)
mydata_5test$y0 <- with(mydata_5test, 3 + .05*GDP_c + .4*EC_c - .8*WTO_c) + 
runif(n = 81, min=min(mydata_5test$Tariff_c),max=max(mydata_5test$Tariff_c)) 
#To create background noise...no effect at all.


lm_test2 <- lm(y0 ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5test)
summary(lm_test2)$r.squared #0.98

mydata_5test$y1 <- with(mydata_5test, y0 + .08*sd(y0))

trueATE <- mean(mydata_5test$y1) - mean(mydata_5test$y0)
trueATE #11.43
.08*sd(mydata_5test$y0) #11.43

#Now, I know the trueATE: 11.43

## Observed outcome: y1 among the treated, y0 among the controls
mydata_5test$Y <- with(mydata_5test, Z*y1 + (1-Z)*y0)

lm_test3 <- lm(Y ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5test) #also use the covariates
summary(lm_test3)$r.squared #0.98


## Notice that this includes all of the true covariates, just not in the correct function, plus it includes some irrelevant covariates
estATEbest_5_5Bs <- coef(OLS_5_5Bs)[["Regime_Bi_c"]] #-2.84
estATEbest_5_5Bs ##-2.84

##Unbiased estimates: includes nothing (no covariates)
estATEunbiased_5_5Bs <- coef(OLS_5_0Bs)[["Regime_Bi_c"]]

estATEunbiased_5_5Bs ##-4.29
```

```{r biassketch, results='hide', eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}
set.seed(2018)

## Bias refers to a relationship between the repeated operation of a procedure and a truth.
## So we have to invent a truth (that is why we **created** potential outcomes for all units ---
## ordinarily we would not observe these quantities).

#Repeat the experiment. Each repetition reveals a different potential outcome for each person. Calculate the estimates from the two different estimators.
#This should show (1) bias and (2) whether one or the other is better in mean squared error terms. What would I do to show that one or another is consistent
#--- especially that one or the other converges to the truth more or less quickly as sample sizes increase?


newEstimate<-function(obsz,dat){
  newExperiment<-function(z){
    #sample(z) is permutating z
    sample(z)
  }
  #create newz
  dat$newz <- newExperiment(obsz)
  ##names(newz) <- row.names(dat)
  #create newY
  dat$newY <- with(dat,newz*y1 + (1-newz)*y0)
  #use three methods to create ATE
  theestATEbest <- coef(lm(newY~newz + Regime_Bi_c + GDP_c + EC_c + WTO_c, data=dat))[["newz"]]
  theestATEunbiased <- coef(lm(newY~newz,data=dat))[["newz"]]
  ## Another method of using covariates
  dat$e1 <- residuals(lm(newY~Regime_Bi_c + GDP_c + EC_c + WTO_c, data=dat))
  theestATEbest2 <- coef(lm(e1~newz,data=dat))[[2]]
  return(c(bestATE=theestATEbest,
           unbiasedATE=theestATEunbiased,
           bestATE2=theestATEbest2))
}

## test, maybe a good idea to test it after we create a function
## newEstimate(obsz=wrkdat$Z,dat=wrkdat)

## An unbiased estimator is one where E[estimator]=Truth. Hmmm.. Is this the right test?
set.seed(1234568)
estdists <- replicate(10000,newEstimate(obsz=mydata_5test$Z,dat=mydata_5test))

```

```{r compare trueATE and the other types of ATE,  eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}

trueATE #11.36
##sampdistmeans<-apply(estdists,1,mean)
#meanATE
sampmeans <- apply(estdists[1:3,],1,mean)
#sampledistmeans: very close to the trueATE
sampmeans

#absolute difference between the estimated mean and the true ATE

bias <- abs(trueATE-apply(estdists[1:3,],1,mean)) # abs(): absoluate positive values: the differences between trueATE and the estimates
apply(estdists[1:3,],1,function(x){ mean( abs(x - trueATE) ) } )
bias
#the standard deviation of the ATE means

##Finding the standard deviation of the estimated ATEs allows us to see if our estimators are efficient.
sd <- apply(estdists[1:3,],1,sd)
sd

#the RMSE of the difference in the true ATE and the estimated ATE

##The RMSE allows us to further assess if the estimators are biased becuase it overesstimates the presence of bias. (to verify)

MSE <- apply(estdists[1:3,],1,function(x){ mean( ( x - trueATE)^2 ) })
MSE
```

```{r trueATE, sampmeans etc, eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}

#install.packages("xtable",repos = "http://cran.us.r-project.org")
library(xtable)

sim <- as.matrix(rbind(trueATE, sampmeans, bias, sd, MSE))
#rownames(rcor) <- names
simtab<-round(sim,2)
simtab
xtable(simtab)
```

```{r xtable print trueATE, sampmeans etc, results = "asis", message = FALSE, eval=T, echo=F, warning = FALSE}
print(xtable::xtable(sim, caption = "Simulation results from different estimates for OLS model"),type = "latex",
      html.table.attributes="border=1", comment=FALSE)
```


```{r plot the bias, RMSE and consistency, results='asis', eval=T, echo=F, message = FALSE, warning =F, cache=TRUE}

plot(density(estdists["bestATE",]),ylim=c(0,.25), main = "Simulation results from estimates for different OLS models")
rug(estdists["bestATE",],col="black",line=0)
lines(density(estdists["unbiasedATE",]),col="blue")
rug(estdists["unbiasedATE",],col="blue",line=.5)
lines(density(estdists[3,]),col="orange")
abline(v=trueATE)

sampdistmeans<-apply(estdists,1,mean)
points(c(sampdistmeans[1:3],estATEunbiased_5_5Bs,estATEbest_5_5Bs), rep(0,5),
       pch=c(17,17,17,2,2),
       cex=1,col=c("black","blue","orange","green","red"))

##bestATE and bestATE2 have a little bias, but it is consistent; 
##unbiasedATE is unbiased, but it is very inefficient and inconsistent.

##We can reduce bias only at a potential increase in variance.

```

The *bestATE* is the estimaor in a *lm* function with all the relevant covariates in the model. The *unbiasedATE* is the estimaor in a *lm* function with no covariates in the model at all. The *bestATE2* is the estimator in a residual-based function.  

  To assess biasedness, I compare sample means of the estimators with the true mean I created in the simulation test. Simply, I can compare the third row, the values of bias for each estimator, and choose the smallest one (bias is denoted as the absolute positive values: the differences between trueATE and the means of the estimates). All of the three estimators are close to the true mean (`r trueATE`) I created in the simulation test. This suggests all three of them are (pretty much) unbiased. In fact, the *bestATE* has the lowest bias out of the three. Its absolute distance to the true mean is the smallest one, with bias equals to `r bias[1]`. The *bestATE2* is slightly biased (with biase equalling to `r bias[3]`).  


  Mean Squared Error (MSE) is the mean of the squared differences between the estimated value and the true value. It combines bias and efficiency to measure the closedness of the estimator to the ture parameter. To assess efficiency, the MSEs for bestATE and bestATE2 are relatively the same, but the bestATE2 behave slightly better (`r MSE[3]` < `r MSE[1]`). The estimators *bestATE* and *bestATE2* are efficient, compared to the *unbiasedATE*. The standard error for *unbiasedATE* is very high (`r sd[2]`), suggesting that this estimate is very inefficient and inconsistent. It does not converge to the true mean at all. From this simulation test, because the bestATE has the lowest bias and is the most efficient one, this estimate is preferred.  


#Statistical Inference and Assessment II

In this section, I present the first Shuffling/permutation: When we have conducted an experiment, and present the readers the treatment effects, we are worried about whether there is a real treatment effect or we get the result by chance. We can surely redo the experiment, but we do not have to. Permutation gives us a way to compute the resampling distribution for test statistics when we assume there is no treatment effect on the outcome of the null hypothesis. If the null hypothesis is true, data after shuffling (the relationships between treatment and outcome are broken) should look like the observed data. P-value gives us information about the probability of getting the observed or more extreme data assuming the null hypothesis is true.


##P-values from the permutation test

P-value tells us how likely I can get the observed treatment effect from my experiment under the no treatment effect null hypothesis. After I have done the hypothetical experiment, I would do a hypothesis testing. Here, in this study, the hypothetical experiment is that countries are "randomly assigned" to be democratic or non-democratic on average over the 1980s and 1990s. The worrisome is the Fisher's sharp null hypothesis: there is a possibility of no effect for all the units in this hypothetical experiment. Instead, I just observe the differences in means by chance. My null hypothesis is there is no treatment effect between the treated and control groups for each unit. In other words, the null hypothesis is there are no differences in the tariff rates between democratic and non-democratic countries.  


```{r randomization and p-value for lm, results='hide', echo = FALSE, message = FALSE, warning=FALSE}


OLS_5_5Bs <- lm(Tariff_c ~ Regime_Bi_c + log(GDP_c) + EC_c + WTO_c, data=mydata_5)
simlmtest_lm5Bs <- summary(OLS_5_5Bs)$coef[2,1]
simlmtest_lm5Bs ##-2.84

#Imagine we re-run the "experiment"
newExp<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lm(y ~ shuffledz+ GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

#set.seed(19900814)
expBeta_lm5Bs <- replicate(10000,newExp(y=mydata_5test$Tariff_c))
mean(expBeta_lm5Bs)  #0.053 treatment effect under null

plot(density(expBeta_lm5Bs), main = "Sampling distribution under null hypothesis and Observed Statistics") #plot the sampling distribution under null hypothesis
rug(mean(expBeta_lm5Bs), 1, col="blue")
abline(v=simlmtest_lm5Bs,lty=2, col="red")

#One-sided p-value
pvalue_lm_os <- as.numeric(as.matrix(summary(expBeta_lm5Bs > simlmtest_lm5Bs)))/length(expBeta_lm5Bs)
pvalue_lm_os #0.1134

#Two-sided p-value
pvalue_lm_ts <- 2*min(mean(expBeta_lm5Bs>=simlmtest_lm5Bs),mean(expBeta_lm5Bs<=simlmtest_lm5Bs)) #0.2268
#pretty close to the two-sided p-value obtained in the lm (0.28). 
p_lmcan <- summary(OLS_5_5Bs)$coef[2,4] 

```

First, I use a test statistic to summarize my observed data from the experiment: `r simlmtest_lm5Bs` is my test statistic of mean difference in tariff rates. Second, I set the null hypothesis of no effects: I create a new experiment and shuffle the labels of countries. Here, I break the relatioships existing in the data structure to create an experiment of no effect. Then, by using the computing power, I replicate the experiments of no effect in the testing on the computer as if I run the experiments 10000 times. Then I observe the differences-in-means `r mean(expBeta_lm5Bs)` under the null hypotehsis, and we compare how likely it is to get the differences-in-means greater than or equal to the observed data. This probability is the p-value `r pvalue_lm_ts`. It means we have `r (pvalue_lm_ts)` (around 1 in 5 replications of the no effect experiment) to produce the values as large as or greater than the estimators in the *lm* function. The p-value here is the probability that value as extreme or more extreme will be observed under the null hypothesis. This probability gives me the information that I may not have much evidence to against the null effect hypothesis, which is the difference between the observed treatment effect and the effect under the null hypothesis is not due to chance.


I use the permutation test to obtain the p-value where the test statistic is calculated based on the data itself under the null hypothesis. The key advantage of this test does not rely on any assumptions of the distribution. In the canned lm function, the standard assumption that the statistic follows a t-distribution gives a p-value of 0.23 (by default). This is in quite good agreement with the p-value I obtained in the permutation test `r pvalue_lm_ts`. But I would not necessarily know beforehand that the two p-values would agree. The following figure shows the null distribution obtained from using the data itself is close to a t-distribution. This can explain why the p-value from the CLT+IID justified test and the p-value from the permutation test is similar.



```{r P-value distribution with Permutation in 50-times simulation, results='asis', echo = FALSE, message = FALSE, warning=FALSE}

newExp<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lm(y ~ shuffledz+ GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

set.seed(19900814)
#expBeta_lm5Bs <- replicate(10000,newExp(y=mydata_5test$Tariff_c))

#mean(expBeta_lm5Bs)#0.053

pvalue_lm <- function(siml,obsTestStat){
  ## return a p-value
  refDistNull<-replicate(siml,newExp(y=mydata_5test$Tariff_c))
  pTwoSided <- 2*min(mean(refDistNull>=obsTestStat),mean(refDistNull<=obsTestStat))
  return(pTwoSided)
}

pvalue_lm (siml=10000, obsTestStat=simlmtest_lm5Bs)

store_pvalue_lm <- replicate(50, 
pvalue_lm (siml=10000, obsTestStat=simlmtest_lm5Bs))

hist(store_pvalue_lm, xlim=c(0.05, 0.3), main=paste("P-value distribution with Permutation in 50-times simulation"))

```

After I calculated the p-value `r pvalue_lm_ts` from a permutation test, I replicated this process for 50 times and calculated 50 different p-values generated from the same process. From this histogram, we can see that the p-values are distributed around from 0.22 to 0.26.


```{r error rate for lm function, results='hide', echo = FALSE, message = FALSE, warning=FALSE}

#Codes to assess the false positive rate of the canned lm function

err_rate<- function(y){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    sim_ps <- summary(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_ps)
}

set.seed(19900814)
results_lm5Bs <- replicate(1000, 
err_rate(y=mydata_5test$Tariff_c))

mean(results_lm5Bs <= .05, na.rm=T) #0.04637

falsepos_lm5Bs <- as.numeric(as.matrix(summary(results_lm5Bs < .05))[3,1])/length(results_lm5Bs) #0.046

falsepos_lm5Bs

```

To check the error rate of the *lm function*, I create a null effect in the error rate test knowing that my null hypothesis is true. If the falsely positive rate is 0.05, this means in 5 out of 100 times, the test faslely rejects the nulls (knowing the null is true but I still reject it). If the false positive rate is close to 0.05, it means the test fulfils its promises. The false positive rate is `r falsepos_lm5Bs`. If we run it a couple of times, the false positive rates are slightly different, but they are around 0.05. The p-value from the built-in lm function has a similar false positive rate to the nominal false positive rate (0.05).

```{r plotting valid test for lm, results='asis', echo = FALSE, message = FALSE, warning=FALSE}


set.seed(800814)
nSims <- 1000

results_lm5Bs <- replicate(1000, 
err_rate(y=mydata_5test$Tariff_c))#get the p-value and store it
#Check power by summing significant p-values and dividing by number of simulations

#The frequency of p-values, or the type 1 error is 0.043. When there is no effect (in the simulation), we can see the p-value is uniformally distributed under the null. This is a valid test because we have set the type 1 error as 0.05, meaning that we can use this test to conclude we have 0.05 chance to make an error saying that there is a true effect where there is actually no true effect.


bars<-20

op <- par(mar = c(5,7,4,4)) #change white-space around graph
hist(results_lm5Bs, breaks=20, xlab="P-values", ylab="number of p-values\n", axes=T,
     main=paste("P-value Distribution under Null Effect in 1000-times Simulation"),
     col="grey", xlim=c(0,1),  ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1))
abline(h=nSims/bars, col = "red", lty=3)


```

In this plot, we know that when there is no true effect, p-values are what is called 'uniformly distributed under the null'. The p-value distribution is basically flat. Every p-value is equally likely when the null hypothesis is true, and every bar in the graph will contain 5% of all the p-values (as indicated by the dotted red line). The first bar is the false positive rate, which is slightly higher than but it is very close to 0.05.

```{r randomization and p-value for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

set.seed(19900814)
lmrob_5_5Bs <- lmrob(Tariff_c ~ Regime_Bi_c + GDP_c + EC_c + WTO_c, data=mydata_5, control = lmrob.control(max.it = 100))
summary(lmrob_5_5Bs)$coefficient #-3.31

simlmrobtest_5Bs <- summary(lmrob_5_5Bs)$coefficient[2,1]
simlmrobtest_5Bs #-3.31

#Imagine we re-run the "experiment" by shuffling the country/treatment
newExp_lmrob<-function(y){
  ## A function to randomly assign treatment effect
shuffledz <- sample(mydata_5test$Z) 
newlmtest <- coef(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))[["shuffledz"]]
  return(newlmtest)
}

set.seed(19900814)
expBeta_lmrob_5Bs <- replicate(10000, newExp_lmrob(y=mydata_5test$Tariff_c))
mean(expBeta_lmrob_5Bs)

plot(density(expBeta_lmrob_5Bs)) #plot the null distribution
rug(mean(expBeta_lmrob_5Bs), 1, col="blue")
abline(v=simlmrobtest_5Bs,lty=2, col="red")

#One-sided p-value
pvalue_lm <- as.numeric(as.matrix(summary(expBeta_lmrob_5Bs < simlmrobtest_5Bs)))/length(expBeta_lmrob_5Bs)
pvalue_lm #0.0325

#Two-sided p-value
pvalue_lmrob_ts <- 2*min(mean(expBeta_lmrob_5Bs>=simlmrobtest_5Bs),mean(expBeta_lmrob_5Bs<=simlmrobtest_5Bs)) #0.065

plmrob_can <- summary(lmrob_5_5Bs)$coefficient[2,4]
#different from the two-sided p-value obtained in the lmrob canned function (0.04).

```

```{r error rate for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

#Codes to assess the false positive rate of the canned lmrob function

err_ratetlmrob<- function(y){
    require(robustbase)
    shuffledz <- sample(mydata_5test$Z) 
  sim_psrob <- summary(lmrob(y ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_psrob)
}

set.seed(23358243)


nSims <- 1000

results_lmrob5Bs <- replicate(nSims, err_ratetlmrob(y=mydata_5test$Tariff_c))#get the p-value and store it

falsepos_lmrob5Bs <- mean(results_lmrob5Bs <= .05, na.rm = T) #0.055

falsepos_lmrob5Bs
#The Type 1 error, the false positive is 0.055.


#Check power by summing significant p-values and dividing by number of simulations

#The frequency of p-values, or the type 1 error is 0.055. When there is no effect (in the simulation), we can see the p-value is uniformally distributed under the null. This is a valid test because we have set the type 1 error as 0.05, meaning that we can use this test to conclude we have 0.05 chance to make an error saying that there is a true effect where there is actually no true effect.

```

```{r plot error rate for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=FALSE}

bars<-20

op <- par(mar = c(5,7,4,4)) #change white-space around graph
hist(results_lmrob5Bs, breaks=20, xlab="P-values", ylab="number of p-values\n", axes=T,
     main=paste("P-value distribution under null effect in the simulation in lmrob"),
     col="grey", xlim=c(0,1),  ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1))
abline(h=nSims/bars, col = "red", lty=3)


```

I also followed the same procedures to calculate the p-value and false positive rates of the *lm rob* function. I summarize the results in the following table. 

```{r prepare table for pvalues, eval = TRUE, echo = FALSE, results='hide', message = FALSE, warning = FALSE, include=FALSE}

#install.packages("xtable",repos = "http://cran.us.r-project.org")
library(xtable)



lmp <- round(c(pvalue_lm_ts, p_lmcan, falsepos_lm5Bs),3)

lmrobp <- round(c(pvalue_lmrob_ts, plmrob_can, falsepos_lmrob5Bs),3)

pvalues <- as.matrix(rbind(lmp, lmrobp))

names_pvalues <- c("Permutation", "t-distritbuion", "False Positive Rate") 
colnames(pvalues) <- names_pvalues

xtable(pvalues)
```

```{r xtable print pvalues, results = "asis", message = FALSE, eval=T, echo=F, warning = FALSE}
print(xtable::xtable(pvalues, caption = "P-values obtained from simulation, t-distribution and their error rates"), type = "latex",
      html.table.attributes="border=1", comment = F)
```


##Power and effect size

When my study has effects, I hope that the test has the power to detect the true effect when the null hypothesis is false. Increasing the power of the test requires bigger sample sizes, or studying larger effects. Here, my sample size is 81, and I want to check which test (of *lm* or *lmrob*) has higher power for different effect sizes. 


```{r effectsize and power for lm, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

## doParallel will not work on all machines. 
library(foreach)
library(doParallel)
cl <- makeCluster(2) ## 2 cores on my machine
registerDoParallel(cl)

set.seed(800814)

power_fn<- function(outcome,effect){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    newoutcome <- outcome - shuffledz*effect
    sim_p<- summary(lm(newoutcome ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_p)
}


#effectsize <- seq(0.1,5, by=0.3)
#results_sim_p <- replicate(1000, power_fn(outcome=mydata4_small$Tariff, effect=0.5))

#somepower <- mean(results_sim_p <= .05) #power
#somepower

######

effectsize <- seq(0.1,5, by=0.3)
somepower <- sapply(seq(0.1,5, by=0.3), function(effectSize){
                     results_sim_p <- replicate (1000, power_fn(outcome=mydata_5test$Tariff_c, effect=effectSize))
                    
                     power <- (mean(results_sim_p <= .05))
                      return(power)
                  })

rbind(effectsize, somepower)

```


```{r effectsize and power for lmrob, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

## doParallel will not work on all machines. 
library(foreach)
library(doParallel)
cl <- makeCluster(2) ## 2 cores on my machine
registerDoParallel(cl)

set.seed(800814)

power_fn_lmrob<- function(outcome,effect){
    shuffledz <- sample(mydata_5test$Z) 
    ## since we know that there is no effect, then we can assess the false positive rate of lm
    newoutcome <- outcome - shuffledz*effect
    sim_p<- summary(lmrob(newoutcome ~ shuffledz + GDP_c + EC_c + WTO_c, data=mydata_5test))$coef[2,4]
return(sim_p)
}

#effectsize <- seq(0.1,5, by=0.3)
#results_sim_p <- replicate(1000, power_fn(outcome=mydata4_small$Tariff, effect=0.5))

#somepower <- mean(results_sim_p <= .05) #power
#somepower

######

effectsize <- seq(0.1,5, by=0.3)

somepower_lmrob <- sapply(seq(0.1,5, by=0.3), function(effectSize){
                     results_sim_p <- replicate (1000, power_fn_lmrob(outcome=mydata_5test$Tariff_c, effect=effectSize))
                    
                     power <- (mean(results_sim_p <= .05, na.rm = T))
                      return(power)
                  })

somepower_lmrob

```

```{r, prepare table power and effectsize, results='hide', echo = FALSE, message = FALSE, warning=FALSE, include=F}

#tablepowers <- rbind(effectsize, somepower, somepower_lmrob)

#install.packages("xtable",repos = "http://cran.us.r-project.org")
library(xtable)

tablepowers <- as.matrix(rbind(effectsize, somepower, somepower_lmrob))
xtable(tablepowers)

```

```{r, print table power and effectsize, results='asis', echo = FALSE, message = FALSE, warning=FALSE}
print(xtable::xtable(tablepowers, caption = "Simulation results of Power and Effect Size for lm and lmrob model"),type = "latex", floating ="false",
      html.table.attributes="border=1", width = "\\textwidth", comment=FALSE)
```

```{r, plot power and effectsize, results='asis', echo = FALSE, message = FALSE, warning=FALSE}

powersdata <- data.frame(effectsize, somepower, somepower_lmrob)

stacked <- with(powersdata,
                data.frame(
                  power = c(somepower, somepower_lmrob),
                  variable = factor(rep(c("powerlm", "powerlmrob"),
                    each = NROW(powersdata))),
                      effectsize = rep(effectsize, 2)))

ggplot(data=stacked, aes(x=effectsize, y=power, colour=variable))+
geom_line()+
geom_hline(yintercept=0.8, linetype="dashed", color="black")+
  labs(title="Power and Effect Size in Simulations", x="Effect Size", y = "Power") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) 

```

I can use simulations to estimate the statistical power of a model. The statistical power is the probability of observing a statistically significant result, if there is a true effect. When there is an effect, I hope that my statistical test is able to detect it. This denotes to high power in my study.   

Cohen describes effect size as the degree to which the null hypothesis is false. In this simulation test, I generate different hypothetical effect sizes (from 0.1 to 5), and I calculate the number of p-values that are lower than 0.05 ("reject the null") when I know there is a true effect (the null is false). When the effect size increases, the powers in both functions also increase. 


For a given sample size, the *lmrob* model has larger statistical power given an effect size. As effect size increases, the power of the *lmrob* model is also increasing faster than that of the *lm* model. To achieve an ideal 80% statistical power, the *lmrob* model requires an effect size larger than 5. 80% statistical power essentially means when there is a true effect, there is 80 percent that I will observe a significant effect. For this *lm* model, I need a bigger effect size to achieve the same level of power as *lmrob* model requires. The tests may have lower power so I may not be able to identify a significant result where there is a real effect.